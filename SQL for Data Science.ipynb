{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.PySpark Connection part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection part for Pyspark and importing required packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pyspark_VS_Pandas\").getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.driver.memory', '4g'), ('spark.executor.memory', '4g'), ('spark.executor.num','6'), ('spark.network.timeout', '1000000')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Reading data into Pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Pyspark dataframe so that we apply SQL scripts for our practice\n",
    "#    1.Read it into Pandas df\n",
    "#    2.convert into pyspark df by defining datatypes of each columns\n",
    "#    [we can use spark.read.format(\"\") option, but it requires additional packages installation so skipped this way]\n",
    "\n",
    "#####################################################################################################\n",
    "#1.STUDENT\n",
    "student_dfpd = pd.read_excel(r'Table_Source\\Student_Placement_Table.xlsx')\n",
    "schema_student = StructType([\\\n",
    "                     StructField(\"ID\",IntegerType(),False),\\\n",
    "                     StructField(\"Name\",StringType(),False),\\\n",
    "                     StructField(\"Gender\",StringType(),False),\\\n",
    "                     StructField(\"DOB\",DateType(),False),\\\n",
    "                     StructField(\"Location\",StringType(),True),\\\n",
    "                     StructField(\"University\",StringType(),False),\\\n",
    "                     StructField(\"Salary\",DoubleType(),False),\\\n",
    "                     StructField(\"Company\",StringType(),False),\\\n",
    "                     StructField(\"Email\",StringType(),False)])\n",
    "\n",
    "student_dfps = spark.createDataFrame(student_dfpd, schema_student)\n",
    "\n",
    "#####################################################################################################\n",
    "#2.UNIVERSITY\n",
    "university_dfpd = pd.read_excel(r'Table_Source\\University_Table.xlsx')\n",
    "schema_university = StructType([\\\n",
    "                     StructField(\"University\",StringType(),False),\\\n",
    "                     StructField(\"MinSalary\",StringType(),False),\\\n",
    "                     StructField(\"PlayGround\",StringType(),False),\\\n",
    "                     StructField(\"Total_Students\",IntegerType(),False)])\n",
    "university_dfps = spark.createDataFrame(university_dfpd, schema_university)\n",
    "\n",
    "#####################################################################################################\n",
    "#3.COMPANY\n",
    "company_dfpd = pd.read_excel(r'Table_Source\\Company_Table.xlsx')\n",
    "schema_company = StructType([\\\n",
    "                     StructField(\"Company\",StringType(),False),\\\n",
    "                     StructField(\"Total_Employes\",IntegerType(),False),\\\n",
    "                     StructField(\"Total_Products\",IntegerType(),False),\\\n",
    "                     StructField(\"Hike_Per_Anum\",IntegerType(),False),\\\n",
    "                     StructField(\"WHF_Office\",StringType(),False)])\n",
    "company_dfps = spark.createDataFrame(company_dfpd, schema_company)\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.PySpark DataFrame to TempView + Columns datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A. Create pyspark dataframe into Temporary view for applying SQL scripts\n",
    "student_dfps.createOrReplaceTempView(\"Student_Table\")\n",
    "university_dfps.createOrReplaceTempView(\"University_Table\")\n",
    "company_dfps.createOrReplaceTempView(\"Company_Table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Student_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student_Table Schema:\n",
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Gender: string (nullable = false)\n",
      " |-- DOB: date (nullable = false)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- University: string (nullable = false)\n",
      " |-- Salary: double (nullable = false)\n",
      " |-- Company: string (nullable = false)\n",
      " |-- Email: string (nullable = false)\n",
      "\n",
      "Total records of Student_Table =  24 \n",
      "\n",
      "Student_Table Data:\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|GGG@hotmail.com|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|  III@gmail.com|\n",
      "|110| JJJ|     M|1990-11-22| Chennai|       NIT|59250.2|   Amazon|JJJ@hotmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|112| LLL|     F|1995-09-20|  Mumbai|       NIT|67900.8|    Apple|  LLL@gmail.com|\n",
      "|113| MMM|     F|1994-10-15|     NaN|       VIT|60200.7|   Google|MMM@hotmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|  PPP@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|  RRR@gmail.com|\n",
      "|119| SSS|     M|1992-10-25|     NaN|       VIT|62900.5|Microsoft|  SSS@gmail.com|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|TTT@hotmail.com|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|  UUU@gmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|WWW@hotmail.com|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print student table schema\n",
    "print(\"Student_Table Schema:\")\n",
    "student_dfps.printSchema()\n",
    "\n",
    "#print total count of records \n",
    "print(\"Total records of Student_Table = \",student_dfps.count(),\"\\n\\nStudent_Table Data:\")\n",
    "\n",
    "#List all the records in table\n",
    "sql_query = \"SELECT * FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. University_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_Table Schema:\n",
      "root\n",
      " |-- University: string (nullable = false)\n",
      " |-- MinSalary: string (nullable = false)\n",
      " |-- PlayGround: string (nullable = false)\n",
      " |-- Total_Students: integer (nullable = false)\n",
      "\n",
      "Total records of University_Table =  7 \n",
      "\n",
      "University_Table Data:\n",
      "+----------+---------+----------+--------------+\n",
      "|University|MinSalary|PlayGround|Total_Students|\n",
      "+----------+---------+----------+--------------+\n",
      "|      IISC|      35K|       YES|           250|\n",
      "|      IIIT|      37K|       YES|           300|\n",
      "|       NIT|      33K|        NO|           280|\n",
      "|       VIT|      36K|        NO|           220|\n",
      "|       MIT|      32K|       YES|           230|\n",
      "|       IIT|      38K|       YES|           240|\n",
      "|      JNTU|      33K|        NO|           250|\n",
      "+----------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print university table schema\n",
    "print(\"University_Table Schema:\")\n",
    "university_dfps.printSchema()\n",
    "\n",
    "#print total count of records \n",
    "print(\"Total records of University_Table = \",university_dfps.count(),\"\\n\\nUniversity_Table Data:\")\n",
    "\n",
    "#List all the records in table\n",
    "sql_query = \"SELECT * FROM University_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Company_Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company_Table Schema:\n",
      "root\n",
      " |-- Company: string (nullable = false)\n",
      " |-- Total_Employes: integer (nullable = false)\n",
      " |-- Total_Products: integer (nullable = false)\n",
      " |-- Hike_Per_Anum: integer (nullable = false)\n",
      " |-- WHF_Office: string (nullable = false)\n",
      "\n",
      "Total records of Company_Table =  6 \n",
      "\n",
      "Company_Table Data:\n",
      "+---------+--------------+--------------+-------------+----------+\n",
      "|  Company|Total_Employes|Total_Products|Hike_Per_Anum|WHF_Office|\n",
      "+---------+--------------+--------------+-------------+----------+\n",
      "|Microsoft|         25000|            50|           25|       WFH|\n",
      "|      TCS|        450000|            20|           20|    Office|\n",
      "|  Infosys|        350000|            19|           18|    Office|\n",
      "|   Google|         30000|            55|           25|       WFH|\n",
      "|   Amazon|        150000|            25|           22|    Office|\n",
      "|    Apple|        200000|            30|           21|       WFH|\n",
      "+---------+--------------+--------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print company table schema\n",
    "print(\"Company_Table Schema:\")\n",
    "company_dfps.printSchema()\n",
    "\n",
    "#print total count of records \n",
    "print(\"Total records of Company_Table = \",company_dfps.count(),\"\\n\\nCompany_Table Data:\")\n",
    "\n",
    "#List all the records in table\n",
    "sql_query = \"SELECT * FROM Company_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.SQL Practice starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @##############################################################@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4A.Select statement + Alias names + Limit + Count(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A1). Print only ID, NAME, GENDER columns\n",
      "+---+----+------+\n",
      "| ID|NAME|GENDER|\n",
      "+---+----+------+\n",
      "|101| AAA|     M|\n",
      "|102| BBB|     F|\n",
      "|103| CCC|     M|\n",
      "|104| DDD|     F|\n",
      "|105| EEE|     M|\n",
      "+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select statement used to select(Print) data \n",
    "#We can give perticular column names to print, or use * to print all columns\n",
    "# Table name = Student_Table\n",
    "#show(5) for limiting records tobe printed\n",
    "\n",
    "print(\"4A1). Print only ID, NAME, GENDER columns\")\n",
    "sql_query=\"\"\"SELECT ID, NAME, GENDER FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SELECT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A2). Print all columns from table\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use * for printing all columns data to console, ##show(5) for limiting records tobe printed\n",
    "print(\"4A2). Print all columns from table\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alias names for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A3). Alias name for ID, Name columns\n",
      "+---------+---------------+\n",
      "|ID_Number|Name_of_Student|\n",
      "+---------+---------------+\n",
      "|      101|            AAA|\n",
      "|      102|            BBB|\n",
      "|      103|            CCC|\n",
      "|      104|            DDD|\n",
      "|      105|            EEE|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming columns with more meaningful names\n",
    "print(\"4A3). Alias name for ID, Name columns\")\n",
    "\n",
    "sql_query=\"\"\"SELECT ID as ID_Number, Name as Name_of_Student FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A4). Limiting number of records tobe printing on console with Limit by 4\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Limiting number of records with LIMIT\n",
    "print(\"4A4). Limiting number of records tobe printing on console with Limit by 4\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table LIMIT 4\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A5). Print total records in given table\n",
      "+-----------+\n",
      "|Total_Count|\n",
      "+-----------+\n",
      "|         24|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count(*) function used to return total number of records that are matching given criteria\n",
    "    #if no filter given for count(*), it will print total records in given table\n",
    "\n",
    "print(\"4A5). Print total records in given table\")\n",
    "\n",
    "sql_query=\"\"\"SELECT count(*)as Total_Count FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A6). Print some sample text using select statement\n",
      "+--------------+\n",
      "|   Column_Name|\n",
      "+--------------+\n",
      "|Hello I am SQL|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selectng some random text using select statement\n",
    "print(\"4A6). Print some sample text using select statement\")\n",
    "\n",
    "sql_query=\"\"\"SELECT 'Hello I am SQL' as Column_Name \"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B.Distinct statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4B1). Without Distinct statement, it will lsit all reocrds in that column(s)\n",
      "+--------+\n",
      "|Location|\n",
      "+--------+\n",
      "|Banglore|\n",
      "|     HYD|\n",
      "| Chennai|\n",
      "|  Mumbai|\n",
      "| Chennai|\n",
      "|     HYD|\n",
      "|  Mumbai|\n",
      "|Banglore|\n",
      "|     HYD|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     NaN|\n",
      "|Banglore|\n",
      "|Banglore|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     NaN|\n",
      "| Chennai|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     HYD|\n",
      "+--------+\n",
      "\n",
      "4B2). With Distinct statement, it will lsit only distinct reocrds in that column(s)\n",
      "+--------+\n",
      "|Location|\n",
      "+--------+\n",
      "| Chennai|\n",
      "|  Mumbai|\n",
      "|     NaN|\n",
      "|     HYD|\n",
      "|Banglore|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct statement used for listing only distinct(different) values in column or list of columns\n",
    "\n",
    "print(\"4B1). Without Distinct statement, it will lsit all reocrds in that column(s)\")\n",
    "sql_query=\"SELECT Location FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)\n",
    "\n",
    "\n",
    "print(\"4B2). With Distinct statement, it will lsit only distinct reocrds in that column(s)\")\n",
    "sql_query=\"SELECT distinct Location FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4C.WHERE clause + BETWEEN + LIKE + IN + AND + OR + IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C1). Print records only from Banglore location\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Where clause used to filter reocrds based on given condition on columns\n",
    "#Below query will filter records from location Banglore\n",
    "\n",
    "print(\"4C1). Print records only from Banglore location\")\n",
    "sql_query=\"SELECT * FROM Student_Table WHERE Location = 'Banglore'\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + BETWEEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C2). Print records only ID range from 105 to 109 Inclusive\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|GGG@hotmail.com|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|  III@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C2). Print records only ID range from 105 to 109 Inclusive\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE ID BETWEEN 105 AND 109\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + LIKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C3). Print records only Company value contains soft\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|119| SSS|     M|1992-10-25|     NaN|       VIT|62900.5|Microsoft|  SSS@gmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C3). Print records only Company value contains soft\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE COMPANY LIKE \"%soft%\" \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C4). Print records only Name in given list(AAA, GGG, KKK)\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|GGG@hotmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C4). Print records only Name in given list(AAA, GGG, KKK)\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE NAME IN ('AAA', 'GGG', 'KKK') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C5). Print records from Banglore location and Microsoft company\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#And should satisfy all conditions\n",
    "print(\"4C5). Print records from Banglore location and Microsoft company\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE (LOCATION ='Banglore' AND COMPANY ='Microsoft') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C6). Print records from Banglore location or Microsoft company\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|119| SSS|     M|1992-10-25|     NaN|       VIT|62900.5|Microsoft|  SSS@gmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OR should satisfy any one conditions, Either of the condition will meet the output\n",
    "print(\"4C6). Print records from Banglore location or Microsoft company\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE (LOCATION ='Banglore' OR COMPANY ='Microsoft') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE + IS NULL + IS NOT NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C7). Print records where University is not null, Here all records will be printed because all rows are having Proper values in University column\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|GGG@hotmail.com|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|  III@gmail.com|\n",
      "|110| JJJ|     M|1990-11-22| Chennai|       NIT|59250.2|   Amazon|JJJ@hotmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|112| LLL|     F|1995-09-20|  Mumbai|       NIT|67900.8|    Apple|  LLL@gmail.com|\n",
      "|113| MMM|     F|1994-10-15|     NaN|       VIT|60200.7|   Google|MMM@hotmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|  PPP@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|  RRR@gmail.com|\n",
      "|119| SSS|     M|1992-10-25|     NaN|       VIT|62900.5|Microsoft|  SSS@gmail.com|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|TTT@hotmail.com|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|  UUU@gmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|WWW@hotmail.com|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Is Null will filter given column having Null values, #Is Not Null will print having proper value\n",
    "# Null is nothing but, missing value in any column(Except Primay key column), it will represent with some meaningful value\n",
    "\n",
    "print(\"4C7). Print records where University is not null, Here all records will be printed because all rows are having Proper values in University column\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE University IS NOT NULL\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D.Order By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ORDER BY ASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D1). Sort by Salary Accending order top 5 records\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|  RRR@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bydefault it is Accending order, for descending order we have to use DESC keyword\n",
    "    #Number: 0 to n bydefault, DESC: n to 0\n",
    "    #Alphabets: A to Z bydefault, DESC: Z to A\n",
    "    \n",
    "print(\"4D1). Sort by Salary Accending order top 5 records\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Salary LIMIT 5\"\"\"\n",
    "spark.sql(sql_query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ORDER BY DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D2). Sort by Name Descending order top 5 records\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|WWW@hotmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|  UUU@gmail.com|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|TTT@hotmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4D2). Sort by Name Descending order top 5 records\")\n",
    "\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Name DESC LIMIT 5\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4E. Upper() + Lower() + Length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4E1). Apply Upper(), Lower(), Length() functions to columns\n",
      "+---------+--------------+--------------+---------------+\n",
      "|  COMPANY|upper(COMPANY)|lower(COMPANY)|length(COMPANY)|\n",
      "+---------+--------------+--------------+---------------+\n",
      "|Microsoft|     MICROSOFT|     microsoft|              9|\n",
      "|    Apple|         APPLE|         apple|              5|\n",
      "|   Amazon|        AMAZON|        amazon|              6|\n",
      "|   Google|        GOOGLE|        google|              6|\n",
      "+---------+--------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upper()-->Convert given column data into upper case data\n",
    "#Lower()-->Convert given column data into lower case data\n",
    "#Length()-->It will print total characters in columns data including spaces\n",
    "\n",
    "print(\"4E1). Apply Upper(), Lower(), Length() functions to columns\")\n",
    "\n",
    "sql_query=\"\"\"SELECT DISTINCT COMPANY,UPPER(COMPANY), LOWER(COMPANY), LENGTH(COMPANY) FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4F. Concatination(||) + BooleanExpression + TRIM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F1). Concatination using || symbol\n",
      "+------------------+\n",
      "|         SelfIntro|\n",
      "+------------------+\n",
      "|I am AAA from IISC|\n",
      "|I am BBB from IIIT|\n",
      "| I am CCC from NIT|\n",
      "| I am DDD from VIT|\n",
      "| I am EEE from IIT|\n",
      "|I am FFF from IIIT|\n",
      "| I am GGG from VIT|\n",
      "| I am HHH from NIT|\n",
      "|I am III from IIIT|\n",
      "| I am JJJ from NIT|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concatination using ||\n",
    "    #This will help to club mutiple columns and some text into single column\n",
    "\n",
    "print(\"4F1). Concatination using || symbol\")\n",
    "\n",
    "sql_query=\"\"\"SELECT 'I am ' || Name || ' from ' || University as SelfIntro FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boolean Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F2). Boolean Expression with some condition\n",
      "+---+----+-------+---------------------+\n",
      "| ID|NAME| SALARY|IsSalaryGraterThan60K|\n",
      "+---+----+-------+---------------------+\n",
      "|101| AAA|55000.5|                false|\n",
      "|102| BBB|76000.2|                 true|\n",
      "|103| CCC|49200.5|                false|\n",
      "|104| DDD|54980.6|                false|\n",
      "|105| EEE|60200.7|                 true|\n",
      "|106| FFF|63100.8|                 true|\n",
      "|107| GGG|60200.7|                 true|\n",
      "|108| HHH|89200.7|                 true|\n",
      "|109| III|66980.8|                 true|\n",
      "|110| JJJ|59250.2|                false|\n",
      "+---+----+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Boolean Expression with some condition:\n",
    "    #THis will print True or False values \n",
    "\n",
    "print(\"4F2). Boolean Expression with some condition\")\n",
    "\n",
    "sql_query=\"\"\"SELECT ID, NAME, SALARY, (Salary > 60000) As IsSalaryGraterThan60K FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F3). Trim() function used to remove extra spaces in column's data\n",
      "+-------------+---------------+-----------+---------------+\n",
      "|  ExtraSpaces|Len_ExtraSpaces|TrimApplied|Len_TrimApplied|\n",
      "+-------------+---------------+-----------+---------------+\n",
      "|   Google    |             13|     Google|              6|\n",
      "+-------------+---------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trim() function used to remove extra spaces in column's data\n",
    "\n",
    "print(\"4F3). Trim() function used to remove extra spaces in column's data\")\n",
    "\n",
    "sql_query=\"\"\"SELECT \n",
    "'   Google    ' AS ExtraSpaces, LENGTH('   Google    ') AS Len_ExtraSpaces,\n",
    "TRIM('   Google    ') AS TrimApplied, LENGTH(TRIM('   Google    ')) AS Len_TrimApplied\n",
    "\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4G.SUBSTRING() + REPLACE() + POSITION() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SUBSTRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4G1).Extract IIIT from IIIT Banglore\n",
      "+-------------+---------------+\n",
      "|   FullColumn|SubstringColumn|\n",
      "+-------------+---------------+\n",
      "|IIIT Banglore|           IIIT|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SUBSTRING() --> function extracts given range text from column \n",
    "\n",
    "print(\"4G1).Extract IIIT from IIIT Banglore\")\n",
    "\n",
    "sql_query=\"\"\"SELECT 'IIIT Banglore' AS FullColumn, SUBSTRING('IIIT Banglore',1,4) AS SubstringColumn \"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4G2). Replace all IIIT to IIIT-B\n",
      "+---+----+----------+--------+\n",
      "| ID|Name|University|Replaced|\n",
      "+---+----+----------+--------+\n",
      "|101| AAA|      IISC|    IISC|\n",
      "|102| BBB|      IIIT|  IIIT-B|\n",
      "|103| CCC|       NIT|     NIT|\n",
      "|104| DDD|       VIT|     VIT|\n",
      "|105| EEE|       IIT|     IIT|\n",
      "|106| FFF|      IIIT|  IIIT-B|\n",
      "|107| GGG|       VIT|     VIT|\n",
      "|108| HHH|       NIT|     NIT|\n",
      "|109| III|      IIIT|  IIIT-B|\n",
      "|110| JJJ|       NIT|     NIT|\n",
      "+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#REPLACE() --> function will replace given text with given replaced value\n",
    "\n",
    "print(\"4G2). Replace all IIIT to IIIT-B\")\n",
    "\n",
    "sql_query=\"\"\"SELECT ID, Name, University, REPLACE(UNIVERSITY, 'IIIT', 'IIIT-B')AS Replaced FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4G3).Print @ symbol position in Email column\n",
      "+---+----+---------------+--------------+\n",
      "| ID|Name|          Email|PositionColumn|\n",
      "+---+----+---------------+--------------+\n",
      "|101| AAA|  AAA@gmail.com|             4|\n",
      "|102| BBB|BBB@hotmail.com|             4|\n",
      "|103| CCC|  CCC@gmail.com|             4|\n",
      "|104| DDD|DDD@hotmail.com|             4|\n",
      "|105| EEE|  EEE@gmail.com|             4|\n",
      "|106| FFF|  FFF@gmail.com|             4|\n",
      "|107| GGG|GGG@hotmail.com|             4|\n",
      "|108| HHH|  HHH@gmail.com|             4|\n",
      "|109| III|  III@gmail.com|             4|\n",
      "|110| JJJ|JJJ@hotmail.com|             4|\n",
      "+---+----+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#POSITION() --> Will print the position number of given pattern or character or string\n",
    "\n",
    "print(\"4G3).Print @ symbol position in Email column\")\n",
    "\n",
    "sql_query=\"\"\"SELECT ID, Name, Email, POSITION('@' IN Email) AS PositionColumn FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4H. Aggregation Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4H1-a).Print Table Based on Salary Descending order \n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|  HHH@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|112| LLL|     F|1995-09-20|  Mumbai|       NIT|67900.8|    Apple|  LLL@gmail.com|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|  III@gmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|119| SSS|     M|1992-10-25|     NaN|       VIT|62900.5|Microsoft|  SSS@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|113| MMM|     F|1994-10-15|     NaN|       VIT|60200.7|   Google|MMM@hotmail.com|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|GGG@hotmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n",
      "4H1-b).Print Maximum value in Salary\n",
      "+----------+\n",
      "|MAX_Salary|\n",
      "+----------+\n",
      "|   89200.7|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sort table based on DOB Descending order\n",
    "\n",
    "print(\"4H1-a).Print Table Based on Salary Descending order \")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Salary DESC LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()\n",
    "\n",
    "\n",
    "print(\"4H1-b).Print Maximum value in Salary\")\n",
    "sql_query=\"\"\"SELECT MAX(Salary) AS MAX_Salary FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4H1-a).Print Table Based on Salary Assending order \n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|  CCC@gmail.com|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|  RRR@gmail.com|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|DDD@hotmail.com|\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|TTT@hotmail.com|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|  PPP@gmail.com|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|WWW@hotmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n",
      "4H1-b).Print Minimum value in Salary\n",
      "+----------+\n",
      "|MIN_Salary|\n",
      "+----------+\n",
      "|   49200.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sort table based on DOB Descending order\n",
    "\n",
    "print(\"4H2-a).Print Table Based on Salary Assending order \")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Salary LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()\n",
    "\n",
    "\n",
    "print(\"4H2-b).Print Minimum value in Salary\")\n",
    "sql_query=\"\"\"SELECT MIN(Salary) AS MIN_Salary FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AVG() + SUM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4H3).AVG(), SUM() functions\n",
      "+-----------------+------------------+\n",
      "|    AverageSalary|         SumSalary|\n",
      "+-----------------+------------------+\n",
      "|61780.98750000001|1482743.7000000002|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AVG() for average, SUM() for sum values based on given condition on column\n",
    "print(\"4H3).AVG(), SUM() functions\")\n",
    "sql_query=\"\"\"SELECT AVG(Salary) AS AverageSalary, SUM(Salary) AS SumSalary FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4I. GROUP BY + HAVING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4I1).Print number of Students working in each company \n",
      "+---------+-----------------------+\n",
      "|  Company|TotalStudentsPerCompany|\n",
      "+---------+-----------------------+\n",
      "|   Google|                      6|\n",
      "|Microsoft|                      8|\n",
      "|    Apple|                      6|\n",
      "|   Amazon|                      4|\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GROUP BY is used to group THE ROWS BASED ON SOME VALUE IN COLUMN \n",
    "    #GROUP BY mainly used in Aggregating functions.\n",
    "\n",
    "print(\"4I1).Print number of Students working in each company \")\n",
    "sql_query=\"\"\"SELECT Company, count(*) TotalStudentsPerCompany FROM Student_Table GROUP BY Company\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4I2).Print Total salary of Students based on company. Note:Round function used\n",
      "+---------+-----------------------+\n",
      "|  Company|TotalStudentsPerCompany|\n",
      "+---------+-----------------------+\n",
      "|   Google|               345653.0|\n",
      "|Microsoft|               495025.0|\n",
      "|    Apple|               387414.0|\n",
      "|   Amazon|               254652.0|\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4I2).Print Total salary of Students based on company. Note:Round function used\")\n",
    "sql_query=\"\"\"SELECT Company, ROUND(SUM(Salary)) TotalStudentsPerCompany FROM Student_Table GROUP BY Company\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4I3).Print list of companies which recruites more than 5 students\n",
      "+---------+-----------------------+\n",
      "|  Company|CompanyHaveMorethan5Stu|\n",
      "+---------+-----------------------+\n",
      "|   Google|                      6|\n",
      "|Microsoft|                      8|\n",
      "|    Apple|                      6|\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Having: Having cluase is used because where cluase cannot be used in Aggregated function. It acts like a filter\n",
    "print(\"4I3).Print list of companies which recruites more than 5 students\")\n",
    "\n",
    "sql_query=\"\"\"SELECT Company, COUNT(*) AS CompanyHaveMorethan5Stu \n",
    "FROM Student_Table \n",
    "GROUP BY Company \n",
    "HAVING CompanyHaveMorethan5Stu>5\"\"\"\n",
    "\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4J. Sub Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4J1).Subquery example. Hint: Only IIT, IIIT, IISC having PlayGround\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|          Email|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|  EEE@gmail.com|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|  PPP@gmail.com|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|  XXX@gmail.com|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|BBB@hotmail.com|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|  FFF@gmail.com|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|  III@gmail.com|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|  NNN@gmail.com|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|TTT@hotmail.com|\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|  AAA@gmail.com|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|  KKK@gmail.com|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|  OOO@gmail.com|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|QQQ@hotmail.com|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|  VVV@gmail.com|\n",
      "+---+----+------+----------+--------+----------+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SubQuery can be nested inside where cluase of another SELECT statement\n",
    "#We can use the comparison operators, such as >, <, or =. \n",
    "#The comparison operator can also be a multiple-row operator, such as IN, ANY, or ALL.\n",
    "\n",
    "print(\"4J1).Subquery example. Hint: Only IIT, IIIT, IISC having PlayGround\")\n",
    "sql_query=\"\"\"SELECT * \n",
    "FROM Student_Table \n",
    "WHERE University IN(\n",
    "                    SELECT University \n",
    "                    FROM University_Table \n",
    "                    WHERE PlayGround = 'YES')\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print(\"4).\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python dict\n",
    "dict1 = {\"Name\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"],\\\n",
    "         \"Weight\":[70,61,83,60,92,69,84,71,77],\\\n",
    "         \"Address\":[\"HYD\",\"Banglore\",\"Chennai\",\"Mumbai\",\"Banglore\",\"Mumbai\",\"Chennai\",\"Banglore\",\"HYD\"],\\\n",
    "         \"DOB\":[\"15-01-1990\", \"19-01-1996\", \"28-02-1999\", \"13-06-1989\", \"15-11-2000\", \"10-12-1995\", \"25-11-1998\", \"15-09-1994\", \"15-01-1996\"],\\\n",
    "         \"Batch\":[2016, 2017, 2018, 2016, 2016, 2017, 2016, 2018, 2017],\\\n",
    "         \"Salary\":[51000.00, 46500.50, 52000.00, 51000.00, 52000.00, 75000.60, 64000.50, 52000.00, 46500.50]         \n",
    "        }\n",
    "\n",
    "#create pandas df\n",
    "dfpd = pd.DataFrame(dict1)\n",
    "dfpd_dtype = {\"Name\":'str', \"Weight\":'int64', \"Address\":'str', \"DOB\":'datetime64', \"Batch\":'int64', \"Salary\":'float64' }\n",
    "dfpd = dfpd.astype(dfpd_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for spark dataframe with Structtype, fields\n",
    "schema = StructType([\\\n",
    "                     StructField(\"Name\", StringType(), True),\\\n",
    "                     StructField(\"Weight\", IntegerType(), True),\\\n",
    "                     StructField(\"Address\", StringType(), True),\\\n",
    "                     StructField(\"DOB\", DateType(), True),\\\n",
    "                     StructField(\"Batch\", IntegerType(), True),\\\n",
    "                     StructField(\"Salary\", DoubleType(), True)])\n",
    "\n",
    "#create spark DF by passing pandas df with above schema\n",
    "dfps = spark.createDataFrame(dfpd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+----------+-----+-------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
